# saving
trained_model_path = trained_models
save_trained = True
load_name = None
load_model = False  #False
n_saved = 1
visdom_path = ./visdom
save_best = True
score_file_name = best_acc

# batch related
epochs = 10
batch_size = 128
drop_last = True
shuffle = True
seed = 42
debug = True
valid_size = 0.1

# logging
print_time = True
log_file = False
log_file_name = None

# training
early_stop = False
cudnn_benchmark = True
learning_rate = 0.001           # source code uses a exponential_decay, not an option in torch's Adam https://discuss.pytorch.org/t/adaptive-learning-rate/320/9

start_visdom = True
use_visdom = True
exp_name = test

# margin loss
use_recon = True    # use reconstruction in the loss, source code suggest no reconstruction for cifar10
alpha = 0.0005      # recon scalar
m_plus = 0.9        # presences factor
m_min = 0.1         # absence factor
use_entropy = False  # use entropy in loss
beta = 0.01         # entropy scalar

# routing
routing_iters = 3
bias_routing = False    # source code has bias True
sparse = nodes_random_0.3-0.3  #note: first use nodes to have more fair percentage
    #nodes_topk_0.3-0.3+nodes_random_0.3-0.3

# network
stdev_W = 0.1
architecture = 32,8;10,16;10,16;10,16

dataset = mnist
model_name = test_caps

compute_activation = True




