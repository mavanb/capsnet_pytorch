# saving
trained_model_path = trained_models
save_trained = True
load_name = None
load_model = False  #False
n_saved = 1
visdom_path = ./visdom
save_best = True
score_file_name = best_acc

# batch related
epochs = 10
batch_size = 128
drop_last = True
shuffle = True
seed = 42
debug = True
valid_size = 0.1

# logging
print_time = True
log_file = False
log_file_name = None

# training
early_stop = False
cudnn_benchmark = True
learning_rate = 0.001           # source code uses a exponential_decay, not an option in torch's Adam https://discuss.pytorch.org/t/adaptive-learning-rate/320/9

start_visdom = False
use_visdom = True
exp_name = test