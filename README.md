# capsnet_pytorch

TODO

### Differences with authors code

* Learning rate decay in Adam optimizer
* Refinement in using one routing iter in primary capsule layer

