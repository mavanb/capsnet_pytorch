# capsnet_pytorch

### Differences with authors code

* Learning rate decay in Adam optimizer
* Refinement in using one routing iter in primary capsule layer
* Bias in routing algo
**** 
